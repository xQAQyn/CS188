# Markov Decision Process

## Definition of MDP

- Markov Decision Process is a non-deterministic search problem

- An MDP is defined by:

  - A set of states $s \in S$
  - A set of actions $a\in A$
  - A transition function $T(s,a,s')$
    - returns the probability from $s$ leads to $s'$ when choosing action $a$, ($P(s'|s,a)$) 
    - also called the model or the dynamics
  - A reward function $R(s,a,s')$
  - A start state
  - Maybe a terminal state

- Policy: we need a optimal policy: $\pi^*: S\to A$

  - The policy $\pi$ gives an action for each state
  - An optimal policy $\pi^*$ maximizes the expected utility
  - The explicit policy defines a reflex agent

- MDP Search Tree:

  ![](https://cdn.luogu.com.cn/upload/image_hosting/6bjh7zxp.png)

  - q-state: having chosen a action, but not seen its resolution

### Utility Function of Sequence

- In general, we prefer 

  - reward to be more to less ($U([3,2,1])\succ U(1,1,2)$)
  - the rewards now to rewards later($U([1,0,0])\succ U([0,0,1])$)

- Discounting: a solution to implement "prefer rewards now to rewards later"

  - idea: the values of rewards decay exponentially, 
  - implement
    - assume the origin value of reward is $1$
    - next time it becomes $\gamma$, where $0<\gamma<1$
    - then it get $\gamma^2$
    - ....
  - result: $U([3,2,1])\succ U([1,2,3])$

- Stationary preference:

  - Theorem: stationary preference satisfies:
    $$
    [a_1,a_2,...]\succ [b_1,b_2,...]
    \leftrightarrow
    [r,a_1,a_2,....]\succ [r,b_1,b_2,...]
    $$

  - we only have two ways to define the utility of stationary preference:

    - Additive utility: $U([r_0,r_1,r_2,...])=r_0+r_1+r_2+...$
    - Discounted utility: $U([r_0,r_1,r_2,...])=r_0+\gamma r_1+\gamma^2r_2+...$

- Infinite utilities:

  - Problem: What if the game last forever? Do we get infinite reward?

  - Solution:

    - method 1: Finite Horizon (just like depth-limit search)

    - method 2: Discounting, the utility will satisfy
      $$
      U([r_0,...,r_\infin])<\frac{R_{max}}{1-\gamma}
      $$

    - method 3: Absorbing State: guarantee that for every policy, a terminal state will be eventually reached

## Solving MDP

- MDP problem:

  - input: a whole definition of an MDP
  - output: a policy

- Hint: important quantities which will be used

  - $V^*(s)$: expected utility started in state $s$ and acting optimally
  - $Q^*(s,a)$: expected utility started in q-state $(s,a)$ and acting optimally after action $a$
  - $\pi^*(s)$: the optimal policy for state $s$

- Value of state:

  - definition: the expected utility under optimal action

  - recursive definition: 
    $$
    V^*(s)=\max_aQ^*(s,a)\\
    Q^*(s,a)=\sum_{s'}(T(s,a,s')\cdot(R(s,a,s')+\gamma V^*(s)))
    $$
    so, we can define $V*(s)$ in this way (The Bellman Equations):
    $$
    V^*(s)=\max_a \sum_s'(T(s,a,s')\cdot (R(s,a,s')+\gamma V^*(s)))
    $$

### Time-limited Value

- Problem:
  - states are repeated
  - trees go forever

#### Value Iteration

- Definition of quantities

  - $V_k(s)$: the optimal value of s if the game ends in $k$ more time steps

- Implement:

  - initially, for all state $s$, $V_0(s)=0$, because there's no time step left

  - then for all $k\ge0$, 
    $$
    V_{k+1}(s)\leftarrow \max_a\sum_{s'}T(s,a,s')(R(s,a,s')+\gamma V_k(s'))
    $$

- Complexity: $O(S^2A)$

  - $S$, the number of states
  - $A$, the number of action

## Algorithm: Policy Methods

### Policy Evaluation (for a fixed policy)

- Utility: the utility of state $s$ under a fixed policy $\pi$ can be defined as:
  $$
  V^\pi(s)=\sum_{s'}T(s,\pi(s),s')[R(s,\pi(s),s')+\gamma V^\pi(s')]
  $$

- Calculation idea : just like Bellman Equation
  $$
  V^\pi_{k+1}(s)=\sum_{s'}T(s,\pi(s),s')[R(s,\pi(s),s')+\gamma V_k^\pi(s')]\\
  V^\pi_0(s)=0
  $$

  - complexity: $O(S^2)$ per iteration

### Policy Extraction

- Compute actions from value: 

  - problem: assume we already have the optimal values $V^*(s)$ for every state $s$

  - solution idea: do a mini-expectimax (one step)

    - calculate the expected value of every action and choose the best one (because you already know all the values, the depth of the search is 1)
      $$
      \pi(s)=\arg_a \max \sum_{s'}T(s,a,s')[R(s,a,s')+\gamma V_k(s')]
      $$

- Compute actions from q-value

  - problem: assume we already have the optimal q-values $Q^*(s,a)$ for every state $s$

  - solution: simply choose the actions leading to maximize the $V(s)$
    $$
    \pi(s)=arg_a\max Q^*(s,a)
    $$

### Policy Iteration

- Idea: 

  - step1: policy evaluation: calculate utility for some fixed policy (not the optimal one) until convergence
  - step2: policy improvement: update policy using one-step look-ahead with resulting converged (still not optimal) utilities as future values
  - repeat steps until policy converges

- Evaluation: iterate the equation until $V_k$ converge
  $$
  V_{k+1}^{\pi_i}(s)=\sum_{s'}T(s,\pi_i(s),s')[R(s,\pi_i(s),s')+\gamma V^{\pi_i}_k(s')]
  $$

- Improvement: For fixed values calculated by evaluation, get a better policy using policy extraction
  $$
  \pi_{i+1}(s)=\arg_a\max\sum_{s'}T(s,a,s')[R(s,a,s')+\gamma V^{\pi_i}(s')]
  $$

