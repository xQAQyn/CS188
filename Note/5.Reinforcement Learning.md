# Reinforcement Learning

## idea

- Important ideas in reinforcement learning:
  - Exploration: you have to try unknown actions to get information
  - Exploitation: eventually, you have to use what you know
    - Regret: even if you learn intelligently, you make mistakes
  - Sampling: because of chance, you have to try things repeatedly
  - Difficulty: learning can be much harder than solving a known MDP

- Basic idea:

  - receive feedback in the form of rewards

  - Agent's utility is defined by the reward function

  - must (learn to ) act so as to maximize the expected rewards

  - All learning is based on the observed samples of outcomes

    ![](https://cdn.luogu.com.cn/upload/image_hosting/wwugqdsn.png)

## Model-Based Learning

- Problem:
  - A set of states $s\in S$
  - A set of actions $a\in A$
  - A model $T(s,a,s')$
  - A reward function $R(s,a,s')$
  - **but**, at this time, we don't know $T$ or $R$ (or both) (so we must try out actions and states to learn)
- Goal: find the optimal policy $\pi$
- Steps:
  - Step1: learn empirical MDP model
    - Count outcomes s' for each (s,a)
    - Normalize to give an estimate of $\hat T(s,a,s')$
    - Discover each $R(s,a,s')$ when we experience $(s,a,s')$
  - Step2: solve the learnt MDP problem (solution in previous note)

## Model-Free Learning

### Passive Reinforcement Learning

- Task:
  - Input: a fixed policy $\pi(s)$
  - Unknown: $T(s,a,s')$ and $R(s,a,s')$
  - Goal: learn the state value
- Important in this case:
  - No choice can be made, just follow the policy

#### Policy Evaluation

- Algo1: direct evaluation
  - idea: simply average all the observed sample values
  - implement:
    - act according to $\pi$
    - record the value of state $s$ every time when visit $s$
    - average the recorded values for each state
  - good:
    - easy to understand
    - doesn't need any knowledge of $T$ and $R$
    - can eventually computes the correct value
  - bad:
    - waste the information about state connections
    - Each state must be learnt separately
    - the above leads the algo takes a long time to learn

- Algo2: sample-based policy evaluation

  - idea: take samples and do bellman-equation-like iteration

  - implement:

    - step1: take some samples
      $$
      sample_1=R(s,\pi(s),s_1')+\gamma V^\pi_{k}(s_1')\\
      sample_2=R(s,\pi(s),s_2')+\gamma V^\pi_{k}(s_2')\\
      sample_3=R(s,\pi(s),s_2')+\gamma V^\pi_{k}(s_3')\\
      ...
      $$

    - step2: iteration
      $$
      V_{k+1}^\pi=\frac{\sum_i sample_i}{n}
      $$

- Algo3: Temporal Difference Learning

  - idea: learn from every experience

    - update $V(s)$ every time when we experience a transition $(s,\pi(s),s')$
    - likely outcome $s'$ will contribute updates more often

  - implement:

    - step1: get a sample
      $$
      sample=R(s,\pi(s),s')+\gamma V^\pi(s)
      $$

    - step2: update value
      $$
      V^\pi(s)=(1-\alpha) V^{\pi}(s)+\alpha\cdot sample
      $$

  - result:

    - the recent sample will be more important
    - can forget the past

#### Policy Improvement

- Problem: we can't use the previous way to update our policy, because we don's have $R$ and $T$

- Solution: just calculate the q-value instead of value, so we can update the policy using
  $$
  \pi(s)=\arg_a\max Q(s,a)
  $$

### Active Reinforcement Learning

- Task:
  - Unknown: $T$ and $R$
  - can choose the actions
  - Goal: learn the optimal policy / values
- Important in this case:
  - Trade off: exploration vs. exploitation
  - Learners make choice

#### Q-learning

- Q-value iteration:
  $$
  Q_{k+1}(s,a)=\sum_{s'}T(s,a,s')(R(s,a,s')+\gamma\max_{a'}Q(s',a'))
  $$

- Implement:

  - step1: get a sample$(s,a,s',r)$
    $$
    sample=R(s,a,s')+\gamma\max_{a'}Q(s',a')\\
    $$

  - step2: update the q-value
    $$
    Q(s,a)=(1-\alpha) Q(s,a)+\alpha\cdot sample
    $$

    - this equation can also be written in this way
      $$
      difference=sample-Q(s,a)\\
      Q(s,a)=Q(s,a)+\alpha\cdot difference
      $$
      

- Important Point:

  - You have to explore enough
  - You have to eventually make the learning rate $\alpha$ small enough, but not decrease it too quickly
  - Basically, in the limit, it doesn't matter how you select actions

## Explore

- Algo1: random action($\varepsilon$-greedy)

  - implement: 
    - every time step, generate a random number
    - with (small) possibility $\varepsilon$, act randomly
    - with (large) possibility $1-\varepsilon$, act on current policy
  - Important point:
    - lower the $\varepsilon$ over time

- Algo2: Exploration Function

  - idea: explore the areas whose badness is not (yet) established, eventually stop exploring

  - implement: 

    - define some function:

      - $f(u,n)$: take an estimate utility(value) and the visit count $n$, returns an optimistic utility, can be defined as follows (k is a constant number)
        $$
        f(u,n)=u+k/(n+1)
        $$

        - $k/(n+1)$ is actually the bonus for exploration, it should be optimistic to encourage exploration
        - the function will decay as $n$ grows and finally converges to the actual limit

      - $N(s,a)$: return the visit count of q-state $(s,a)$

    - Modified the Q-update:
      $$
      Q(s,a)=(1-\alpha) Q(s,a)+\alpha(R(s,a,s')+\gamma \max_{a'}f(Q(s',a'),N(s',a')))
      $$

  - result: the state with less visit will have high q-value, so the agent will be willing to explore the less visited state

### Regret

- Even if you learn the optimal policy, you still make mistakes
- Regret is a measure of your total mistake cost
- Minimize Regret:
  - Finally get the optimal policy
  - The process of learning runs as quickly as possible

## Approximate Q-learning

- Problem: sometimes the state space is too large to do regular q-value iteration

- Solution: describe a state using a vector of features

- Implement1: ($f$ is a feature of the state)
  $$
  V(s)=w_1f_1(s)+w_2f_2(s)+...+w_nf_n(s)\\
  Q(s,a)=w_1f_1(s,a)+w_2f_2(s,a)+...+w_nf_n(s)
  $$

  - Advantage: our experience is summed up in a few powerful numbers

  - Disadvantage: some states may share features but have different value, in this way we can't tell them

  - Adjust : at this time, we will not store all the q-value(because it is too much), so if we want to update the q-value, we should update the weight
    $$
    difference=sample-Q(s,a)\\
    w_i= w_i+\alpha\cdot difference\cdot f_i(s,a)
    $$

    - the way of updating weight is based on Least Squares